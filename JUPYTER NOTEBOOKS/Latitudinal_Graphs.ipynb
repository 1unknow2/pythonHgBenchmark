{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brittany/anaconda3/envs/geoschem/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/brittany/anaconda3/envs/geoschem/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xbpch\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import colorbar, colors\n",
    "import statistics\n",
    "from sklearn.metrics import r2_score\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brittany/anaconda3/envs/geoschem/lib/python3.6/site-packages/pandas/io/parsers.py:710: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    }
   ],
   "source": [
    "# Name the xbpch files for the reference and new models\n",
    "OLD = '/home/brittany/Documents/HG/pythonHgBenchmark/trac_avg.geosfp_2x25_Hg.v12-01.bpch'\n",
    "NEW = '/home/brittany/Documents/HG/pythonHgBenchmark/trac_avg.geosfp_2x25_Hg.v12-01.new.bpch'\n",
    "\n",
    "\n",
    "# Upload/open the data from the models \n",
    "OLD_ds = (xbpch.open_bpchdataset(OLD))\n",
    "NEW_ds = (xbpch.open_bpchdataset(NEW))\n",
    "Dataset_OLD, Dataset_NEW= OLD_ds, NEW_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LatitudinalGraphs(Dataset_OLD, Dataset_NEW):\n",
    "    # Import the observed data from the sites     \n",
    "    Hgobs = pd.read_csv('data/TGMSiteMonthly.csv',  skiprows=[0], na_values=(-9999))\n",
    "    Hgobs.columns=['SiteID', 'Lat', 'Lon','Month', 'Year', 'Concentration', 'Standard deviation']\n",
    " \n",
    "    # Create a dictionary for the levels of the model who's surfaces are not at 0\n",
    "    def levels(SiteID):\n",
    "        level = {\n",
    "        'ZEP': 3,\n",
    "        'AND': 2,\n",
    "        'MWA': 1,\n",
    "        'MLO': 18,\n",
    "        'MBO': 16,\n",
    "        'NamCo': 18,\n",
    "        'LLN':16,\n",
    "            }\n",
    "        return level.get(SiteID.upper(), 0)\n",
    "\n",
    "\n",
    "    # Make a variable for the unit conversion factor to obtain ng/m^3\n",
    "    Unit_Conversion= 8.93\n",
    "    # Make arrays of SiteIDs for the Arctic, Antarctic and Northern and Souther Mid Latitudes.\n",
    "    Arctic=np.array(['ALT', 'VRS', 'ZEP', 'AND', 'PAL'])\n",
    "    SouthMidLat=np.array(['CPT', 'AMS', 'BAR'])\n",
    "    Antarctic= np.array(['TRO', 'DDU', 'DMC'])\n",
    "    NorthMidLat= np.array(['MHD', 'UDH', 'KEJ',  'HTW', 'PNY', 'ATN', 'YKV', 'GRB']) #'EBG',\n",
    "    \n",
    "    # Create numpy zeros for the amount of items in each list.\n",
    "    Arc_ds=np.zeros(len(Arctic))\n",
    "    SML_ds=np.zeros(len(SouthMidLat))\n",
    "    Ant_ds=np.zeros(len(Antarctic))\n",
    "    NML_ds= np.zeros(len(NorthMidLat))\n",
    "\n",
    "\n",
    "    # Select the list of numpy zeros for the Arctic and extract the data from each site, creating a new data.\n",
    "    for i in range (len(Arctic)):\n",
    "       \n",
    "\n",
    "        Arc_ds= Hgobs[Hgobs.SiteID==Arctic[i]].reset_index()\n",
    "   \n",
    "        if i==0:\n",
    "\n",
    "            All_Arctic_ds = Arc_ds\n",
    "\n",
    "        else:\n",
    "\n",
    "            All_Arctic_ds = pd.concat([All_Arctic_ds,Arc_ds])\n",
    "\n",
    "    # Calculate the mean and stanadard deviation for each month.\n",
    "    Arc_graph = All_Arctic_ds.groupby('Month').mean()\n",
    "    Arc_graph_SD = All_Arctic_ds.groupby('Month').std() \n",
    "    # Select all unique latitudes and longitudes from the dataset.\n",
    "    Arc_lat= All_Arctic_ds.Lat.unique()\n",
    "    Arc_lon= All_Arctic_ds.Lon.unique()\n",
    "\n",
    "    # Create a dataset for the total TGM at each site for the reference and new models.\n",
    "    for i in range (len(Arctic)): \n",
    "        Arc_OLD_Hg0 =((Dataset_OLD['IJ_AVG_S_Hg0'].isel(lev=levels(Arctic[i]) ) * Unit_Conversion))                              \n",
    "        Arc_OLD_Hg2 =((Dataset_OLD['IJ_AVG_S_Hg2'].isel(lev=levels(Arctic[i])) * Unit_Conversion) )               \n",
    "        Arc_TGM_Old = (Arc_OLD_Hg0 + Arc_OLD_Hg2)\n",
    "        Arc_OLD_mod= (Arc_TGM_Old.sel(lat=Arc_lat[i], lon=Arc_lon[i],  method='nearest'))\n",
    "    \n",
    "        Arc_NEW_Hg0 =((Dataset_NEW['IJ_AVG_S_Hg0'].isel(lev=levels(Arctic[i]) )) * Unit_Conversion)\n",
    "        Arc_NEW_Hg2 =((Dataset_NEW['IJ_AVG_S_Hg2'].isel(lev=levels(Arctic[i]) )) * Unit_Conversion)\n",
    "        Arc_TGM_New =( Arc_NEW_Hg0 + Arc_NEW_Hg2)\n",
    "        Arc_NEW_mod= (Arc_TGM_New.sel(lat=Arc_lat[i], lon=Arc_lon[i],  method='nearest'))\n",
    "        if i==0:\n",
    "\n",
    "            Arc_DS_OLD = Arc_OLD_mod\n",
    "            Arc_DS_NEW = Arc_NEW_mod\n",
    "        else:\n",
    "\n",
    "            Arc_DS_OLD= xr.concat([Arc_DS_OLD,Arc_OLD_mod])\n",
    "            Arc_DS_NEW= xr.concat([Arc_DS_NEW,Arc_NEW_mod])\n",
    "   # Calculate the mean and standard deviations for the reference and new models.\n",
    "    Arc_meanmod_OLD=Arc_DS_OLD.mean('concat_dims')\n",
    "    Arc_stdevmod_OLD= Arc_DS_OLD.std('concat_dims')\n",
    "    \n",
    "    Arc_meanmod_NEW= Arc_DS_NEW.mean('concat_dims')\n",
    "    Arc_stdevmod_NEW= Arc_DS_NEW.std('concat_dims')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # Select the list of numpy zeros for the South Mid Latitudes and extract the data from each site, creating a\n",
    "    # new data.\n",
    "    for i in range (len(SouthMidLat)):\n",
    "        SML_ds= Hgobs[Hgobs.SiteID==SouthMidLat[i]].reset_index()\n",
    "        if i==0:\n",
    "\n",
    "            All_SML_ds = SML_ds\n",
    "\n",
    "        else:\n",
    "\n",
    "            All_SML_ds = pd.concat([All_SML_ds,SML_ds])\n",
    "        \n",
    "    # Calculate the mean and stanadard deviation for each month.        \n",
    "    SML_graph = All_SML_ds.groupby('Month').mean()\n",
    "    SML_graph_SD = All_SML_ds.groupby('Month').std()\n",
    "    # Select all unique latitudes and longitudes from the dataset.\n",
    "    SML_lat= All_SML_ds.Lat.unique()\n",
    "    SML_lon= All_SML_ds.Lon.unique()\n",
    "    \n",
    "    # Create a dataset for the total TGM at each site for the reference and new models.    \n",
    "    for i in range (len(SouthMidLat)): \n",
    "        SML_OLD_Hg0 =((Dataset_OLD['IJ_AVG_S_Hg0'].isel(lev=levels(SouthMidLat[i]) ) * Unit_Conversion))                              \n",
    "        SML_OLD_Hg2 =((Dataset_OLD['IJ_AVG_S_Hg2'].isel(lev=levels(SouthMidLat[i])) * Unit_Conversion) )               \n",
    "        SML_TGM_Old = (SML_OLD_Hg0 + SML_OLD_Hg2)\n",
    "        SML_OLD_mod= (SML_TGM_Old.sel(lat=SML_lat[i], lon=SML_lon[i],  method='nearest'))\n",
    "    \n",
    "        SML_NEW_Hg0 =((Dataset_NEW['IJ_AVG_S_Hg0'].isel(lev=levels(SouthMidLat[i]) )) * Unit_Conversion)\n",
    "        SML_NEW_Hg2 =((Dataset_NEW['IJ_AVG_S_Hg2'].isel(lev=levels(SouthMidLat[i]) )) * Unit_Conversion)\n",
    "        SML_TGM_New =( SML_NEW_Hg0 + SML_NEW_Hg2)\n",
    "        SML_NEW_mod= (SML_TGM_New.sel(lat=SML_lat[i], lon=SML_lon[i],  method='nearest'))\n",
    "        if i==0:\n",
    "\n",
    "            SML_DS_OLD = SML_OLD_mod\n",
    "           # TGM_New_Arc= TGM_New\n",
    "            SML_DS_NEW = SML_NEW_mod\n",
    "        else:\n",
    "\n",
    "            SML_DS_OLD= xr.concat([SML_DS_OLD,SML_OLD_mod])\n",
    "            SML_DS_NEW= xr.concat([SML_DS_NEW,SML_NEW_mod])\n",
    "    # Calculate the mean and standard deviations for the reference and new models.\n",
    "    SML_meanmod_OLD=SML_DS_OLD.mean('concat_dims')\n",
    "    SML_stdevmod_OLD= SML_DS_OLD.std('concat_dims')\n",
    "    \n",
    "    SML_meanmod_NEW= SML_DS_NEW.mean('concat_dims')\n",
    "    SML_stdevmod_NEW= SML_DS_NEW.std('concat_dims')  \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "    # Select the list of numpy zeros for the Antarctic and extract the data from each site, creating a new data.    \n",
    "    for i in range (len(Antarctic)):\n",
    "        Ant_ds= Hgobs[Hgobs.SiteID==Antarctic[i]].reset_index()\n",
    "        if i==0:\n",
    "\n",
    "            All_Ant_ds = Ant_ds\n",
    "\n",
    "        else:\n",
    "\n",
    "            All_Ant_ds = pd.concat([All_Ant_ds,Ant_ds])    \n",
    "    \n",
    "    # Calculate the mean and stanadard deviation for each month.        \n",
    "    Ant_graph = All_Ant_ds.groupby('Month').mean()\n",
    "    Ant_graph_SD = All_Ant_ds.groupby('Month').std()\n",
    "    # Select all unique latitudes and longitudes from the dataset.\n",
    "    Ant_lat= All_Ant_ds.Lat.unique()\n",
    "    Ant_lon= All_Ant_ds.Lon.unique()\n",
    "    \n",
    "    # Create a dataset for the total TGM at each site for the reference and new models.\n",
    "    for i in range (len(Antarctic)): \n",
    "        Ant_OLD_Hg0 =((Dataset_OLD['IJ_AVG_S_Hg0'].isel(lev=levels(Antarctic[i]) ) * Unit_Conversion))                              \n",
    "        Ant_OLD_Hg2 =((Dataset_OLD['IJ_AVG_S_Hg2'].isel(lev=levels(Antarctic[i])) * Unit_Conversion) )               \n",
    "        Ant_TGM_Old = (Ant_OLD_Hg0 + Ant_OLD_Hg2)\n",
    "        Ant_OLD_mod= (Ant_TGM_Old.sel(lat=Ant_lat[i], lon=Ant_lon[i],  method='nearest'))\n",
    "    \n",
    "        Ant_NEW_Hg0 =((Dataset_NEW['IJ_AVG_S_Hg0'].isel(lev=levels(Antarctic[i]) )) * Unit_Conversion)\n",
    "        Ant_NEW_Hg2 =((Dataset_NEW['IJ_AVG_S_Hg2'].isel(lev=levels(Antarctic[i]) )) * Unit_Conversion)\n",
    "        Ant_TGM_New =( Ant_NEW_Hg0 + Ant_NEW_Hg2)\n",
    "        Ant_NEW_mod= (Ant_TGM_New.sel(lat=Ant_lat[i], lon=Ant_lon[i],  method='nearest'))\n",
    "        if i==0:\n",
    "\n",
    "            Ant_DS_OLD = Ant_OLD_mod\n",
    "           # TGM_New_Arc= TGM_New\n",
    "            Ant_DS_NEW = Ant_NEW_mod\n",
    "        else:\n",
    "\n",
    "            Ant_DS_OLD= xr.concat([Ant_DS_OLD,Ant_OLD_mod])\n",
    "            Ant_DS_NEW= xr.concat([Ant_DS_NEW,Ant_NEW_mod])\n",
    "            \n",
    "    # Calculate the mean and standard deviations for the reference and new models.   \n",
    "    Ant_meanmod_OLD=Ant_DS_OLD.mean('concat_dims')\n",
    "    Ant_stdevmod_OLD= Ant_DS_OLD.std('concat_dims')\n",
    "    \n",
    "    Ant_meanmod_NEW= Ant_DS_NEW.mean('concat_dims')\n",
    "    Ant_stdevmod_NEW= Ant_DS_NEW.std('concat_dims')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Select the list of numpy zeros for the North Mid Latitudes and extract the data from each site, creating a\n",
    "    # new data.\n",
    "    for i in range (len(NorthMidLat)):\n",
    "        NML_ds= Hgobs[Hgobs.SiteID==NorthMidLat[i]].reset_index()\n",
    "        if i==0:\n",
    "\n",
    "            All_NML_ds = NML_ds\n",
    "\n",
    "        else:\n",
    "\n",
    "            All_NML_ds = pd.concat([All_NML_ds,NML_ds])\n",
    "\n",
    "    \n",
    "    # Calculate the mean and stanadard deviation for each month.        \n",
    "    NML_graph = All_NML_ds.groupby('Month').mean()\n",
    "    NML_graph_SD = All_NML_ds.groupby('Month').std()\n",
    "    # Select all unique latitudes and longitudes from the dataset.\n",
    "    NML_lat= All_NML_ds.Lat.unique()\n",
    "    NML_lon= All_NML_ds.Lon.unique()\n",
    "    \n",
    "    # Create a dataset for the total TGM at each site for the reference and new models.    \n",
    "    for i in range (len(NorthMidLat)): \n",
    "        NML_OLD_Hg0 =((Dataset_OLD['IJ_AVG_S_Hg0'].isel(lev=levels(NorthMidLat[i]) ) * Unit_Conversion))                              \n",
    "        NML_OLD_Hg2 =((Dataset_OLD['IJ_AVG_S_Hg2'].isel(lev=levels(NorthMidLat[i])) * Unit_Conversion) )               \n",
    "        NML_TGM_Old = (NML_OLD_Hg0 + NML_OLD_Hg2)\n",
    "        NML_OLD_mod= (NML_TGM_Old.sel(lat=NML_lat[i], lon=NML_lon[i],  method='nearest'))\n",
    "    \n",
    "        NML_NEW_Hg0 =((Dataset_NEW['IJ_AVG_S_Hg0'].isel(lev=levels(NorthMidLat[i]) )) * Unit_Conversion)\n",
    "        NML_NEW_Hg2 =((Dataset_NEW['IJ_AVG_S_Hg2'].isel(lev=levels(NorthMidLat[i]) )) * Unit_Conversion)\n",
    "        NML_TGM_New =( NML_NEW_Hg0 + NML_NEW_Hg2)\n",
    "        NML_NEW_mod= (NML_TGM_New.sel(lat=NML_lat[i], lon=NML_lon[i],  method='nearest'))\n",
    "        if i==0:\n",
    "\n",
    "            NML_DS_OLD = NML_OLD_mod\n",
    "           # TGM_New_Arc= TGM_New\n",
    "            NML_DS_NEW = NML_NEW_mod\n",
    "        else:\n",
    "\n",
    "            NML_DS_OLD= xr.concat([NML_DS_OLD,NML_OLD_mod])\n",
    "            NML_DS_NEW= xr.concat([NML_DS_NEW,NML_NEW_mod])\n",
    "            \n",
    "    # Calculate the mean and standard deviations for the reference and new models.\n",
    "    print (NML_DS_OLD)\n",
    "    NML_meanmod_OLD=NML_DS_OLD.mean('concat_dims')\n",
    "    print (NML_meanmod_OLD)\n",
    "    NML_stdevmod_OLD= NML_DS_OLD.std('concat_dims')\n",
    "    \n",
    "    NML_meanmod_NEW= NML_DS_NEW.mean('concat_dims')\n",
    "    NML_stdevmod_NEW= NML_DS_NEW.std('concat_dims')\n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    # Plot the four graphs as subplots.\n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "                                                # ARCTIC #\n",
    "    # Convert the time data from a float to a string, specifying months for graph labels\n",
    "    Arc_ds.index=pd.to_datetime(Arc_ds.Month, format='%m')\n",
    "    # Add a subplot\n",
    "    ax= plt.subplot(221)\n",
    "    # Plot the observations and their error.\n",
    "    plt.errorbar(Arc_ds.Month, Arc_graph.Concentration,yerr=Arc_graph_SD.Concentration, color='k')\n",
    "    # Plot the reference and new models on the same graph with their errors.\n",
    "    ax.errorbar(Arc_ds.Month,Arc_meanmod_OLD ,yerr=Arc_stdevmod_OLD, color='Blue')\n",
    "    ax.errorbar(Arc_ds.Month, Arc_meanmod_NEW,yerr=Arc_stdevmod_NEW, color='Red')\n",
    "    # Label the x and y axis. \n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('TGM (ng/m3)')\n",
    "    # Add a legend\n",
    "    plt.legend([ 'Observations','Reference Model','New Model' ])\n",
    "    # Add a title.\n",
    "    plt.title(\"Arctic\")\n",
    "    # Set ticks to every month \n",
    "    ax.set_xticks(Arc_ds.Month)\n",
    "    # Set tick labels to month names\n",
    "    ax.set_xticklabels(Arc_ds.index.strftime('%b'))\n",
    "    \n",
    "                                                # ANTARCTIC #    \n",
    "    # Convert the time data from a float to a string, specifying months for graph labels\n",
    "    Ant_ds.index=pd.to_datetime(Ant_ds.Month, format='%m')    \n",
    "    # Add a subplot.\n",
    "    ax= plt.subplot(222)\n",
    "    # Plot the observations and their error.\n",
    "    plt.errorbar(Ant_ds.Month, Ant_graph.Concentration,yerr=Ant_graph_SD.Concentration, color='k')\n",
    "    # Plot the reference and new models on the same graph with their errors.\n",
    "    ax.errorbar(Ant_ds.Month,Ant_meanmod_OLD ,yerr=Ant_stdevmod_OLD, color='Blue')\n",
    "    ax.errorbar(Ant_ds.Month, Ant_meanmod_NEW,yerr=Ant_stdevmod_NEW, color='Red')\n",
    "    # Label the x and y axis. \n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('TGM (ng/m3)')\n",
    "    # Add a legend    \n",
    "    plt.legend([ 'Observations','Reference Model','New Model' ])\n",
    "    # Add a title.\n",
    "    plt.title(\"Antarctic\")\n",
    "    # Set ticks to every month \n",
    "    ax.set_xticks(Ant_ds.Month)    \n",
    "    # Set tick labels to month names\n",
    "    ax.set_xticklabels(Ant_ds.index.strftime('%b'))\n",
    "    \n",
    "    \n",
    "                                        # NORTHERN MID LATITUDES #     \n",
    "    # Convert the time data from a float to a string, specifying months for graph labels\n",
    "    NML_ds.index=pd.to_datetime(NML_ds.Month, format='%m')\n",
    "    # Add a subplot. \n",
    "    ax= plt.subplot(223)\n",
    "    # Plot the observations and their error.\n",
    "    plt.errorbar(NML_ds.Month, NML_graph.Concentration,yerr=NML_graph_SD.Concentration, color='k')\n",
    "    # Plot the reference and new models on the same graph with their errors.\n",
    "    ax.errorbar(NML_ds.Month,NML_meanmod_OLD ,yerr=NML_stdevmod_OLD, color='Blue')\n",
    "    ax.errorbar(NML_ds.Month, NML_meanmod_NEW,yerr=NML_stdevmod_NEW, color='Red')\n",
    "    # Label the x and y axis.     \n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('TGM (ng/m3)')\n",
    "    # Add a legend\n",
    "    plt.legend([ 'Observations','Reference Model','New Model' ])\n",
    "    # Add a title.\n",
    "    plt.title(\"Northern Mid Latitudes\")\n",
    "    # Set ticks to every month \n",
    "    ax.set_xticks(NML_ds.Month)\n",
    "    # Set tick labels to month names\n",
    "    ax.set_xticklabels(NML_ds.index.strftime('%b'))\n",
    "    \n",
    "                                        # SOUTHERN MID LATITUDES #     \n",
    "    # Convert the time data from a float to a string, specifying months for graph labels\n",
    "    SML_ds.index=pd.to_datetime(SML_ds.Month, format='%m')\n",
    "    # Add a subplot.\n",
    "    ax= plt.subplot(224)\n",
    "    # Plot the observations and their error.\n",
    "    plt.errorbar(SML_ds.Month, SML_graph.Concentration,yerr=SML_graph_SD.Concentration, color='k')\n",
    "    # Plot the reference and new models on the same graph with their errors.\n",
    "    ax.errorbar(SML_ds.Month,SML_meanmod_OLD ,yerr=SML_stdevmod_OLD, color='Blue')\n",
    "    ax.errorbar(SML_ds.Month, SML_meanmod_NEW,yerr=SML_stdevmod_NEW, color='Red')\n",
    "    # Label the x and y axis. \n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('TGM (ng/m3)')\n",
    "    # Add a legend\n",
    "    plt.legend([ 'Observations','Reference Model','New Model' ])\n",
    "    # Add a title.\n",
    "    plt.title(\"Southern Mid Latitudes\")\n",
    "    # Set ticks to every month \n",
    "    ax.set_xticks(SML_ds.Month)    \n",
    "    # Set tick labels to month names\n",
    "    ax.set_xticklabels(SML_ds.index.strftime('%b'))\n",
    "  \n",
    "\n",
    "    \n",
    "    \n",
    "     # Show the 4 subplots \n",
    "    LatGraph= plt.show()\n",
    "    return LatGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/TGMSiteMonthly.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-48efdd69496b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLatitudinalGraphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOLD_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNEW_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b3626f7016f2>\u001b[0m in \u001b[0;36mLatitudinalGraphs\u001b[0;34m(Dataset_OLD, Dataset_NEW)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLatitudinalGraphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset_OLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset_NEW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Import the observed data from the sites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mHgobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/TGMSiteMonthly.csv'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m9999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mHgobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SiteID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Lon'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Concentration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Standard deviation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoschem/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoschem/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoschem/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoschem/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geoschem/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/TGMSiteMonthly.csv' does not exist"
     ]
    }
   ],
   "source": [
    "LatitudinalGraphs(OLD_ds, NEW_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NML_DS_OLD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-ebee3779ac43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;31m#      NML_DS_NEW= xr.concat([NML_DS_NEW,NML_NEW_mod])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mNML_NEW_mod\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNML_DS_OLD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNML_lat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNML_lon\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mNML_OLD_mod\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNML_DS_NEW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNML_lat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNML_lon\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNML_NEW_mod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NML_DS_OLD' is not defined"
     ]
    }
   ],
   "source": [
    "    Hgobs = pd.read_csv('data/TGMSiteMonthly.csv',  skiprows=[0], na_values=(-9999))\n",
    "    Hgobs.columns=['SiteID', 'Lat', 'Lon','Month', 'Year', 'Concentration', 'Standard deviation']\n",
    " \n",
    "    # Create a dictionary for the levels of the model who's surfaces are not at 0\n",
    "    def levels(SiteID):\n",
    "        level = {\n",
    "        'ZEP': 3,\n",
    "        'AND': 2,\n",
    "        'MWA': 1,\n",
    "        'MLO': 18,\n",
    "        'MBO': 16,\n",
    "        'NamCo': 18,\n",
    "        'LLN':16,\n",
    "            }\n",
    "        return level.get(SiteID.upper(), 0)\n",
    "\n",
    "\n",
    "    # Make a variable for the unit conversion factor to obtain ng/m^3\n",
    "    Unit_Conversion= 8.93\n",
    "    # Make arrays of SiteIDs for the Arctic, Antarctic and Northern and Souther Mid Latitudes.\n",
    "\n",
    "    NorthMidLat= np.array(['MHD', 'UDH', 'KEJ',  'HTW', 'PNY', 'ATN', 'YKV', 'GRB']) #'EBG',\n",
    "    for i in range (len(NorthMidLat)):\n",
    "        NML_ds= Hgobs[Hgobs.SiteID==NorthMidLat[i]].reset_index()\n",
    "       \n",
    "        \n",
    "        NML_OLD_Hg0 =((Dataset_OLD['IJ_AVG_S_Hg0'].isel(lev=levels(NorthMidLat[i]) ) * Unit_Conversion))                              \n",
    "        NML_OLD_Hg2 =((Dataset_OLD['IJ_AVG_S_Hg2'].isel(lev=levels(NorthMidLat[i])) * Unit_Conversion) )               \n",
    "        NML_TGM_Old = (NML_OLD_Hg0 + NML_OLD_Hg2)\n",
    "        \n",
    "        \n",
    "        NML_NEW_Hg0 =((Dataset_NEW['IJ_AVG_S_Hg0'].isel(lev=levels(NorthMidLat[i]) )) * Unit_Conversion)\n",
    "        NML_NEW_Hg2 =((Dataset_NEW['IJ_AVG_S_Hg2'].isel(lev=levels(NorthMidLat[i]) )) * Unit_Conversion)\n",
    "        NML_TGM_New =( NML_NEW_Hg0 + NML_NEW_Hg2)\n",
    "       \n",
    "        if i==0:\n",
    "\n",
    "            All_NML_ds = NML_ds\n",
    "            NML_DS_OLD = NML_TGM_Old\n",
    "        \n",
    "            NML_DS_NEW = NML_TGM_New\n",
    "\n",
    "        else:\n",
    "\n",
    "            All_NML_ds = pd.concat([All_NML_ds,NML_ds])\n",
    "            NML_DS_OLD= xr.concat([NML_DS_OLD,NML_TGM_Old])\n",
    "            NML_DS_NEW= xr.concat([NML_DS_NEW,NML_TGM_New])\n",
    "            \n",
    "    NML_lat= All_NML_ds.Lat.unique()\n",
    "    NML_lon= All_NML_ds.Lon.unique()\n",
    "    \n",
    "#    if i==0:\n",
    "#\n",
    " #       NML_DS_OLD = NML_OLD_mod\n",
    "  #      \n",
    "   #     NML_DS_NEW = NML_NEW_mod\n",
    "   # else:\n",
    "#\n",
    " #       NML_DS_OLD= xr.concat([NML_DS_OLD,NML_OLD_mod])\n",
    "  #      NML_DS_NEW= xr.concat([NML_DS_NEW,NML_NEW_mod])    \n",
    "    \n",
    "    NML_NEW_mod= (NML_DS_OLD.sel(lat=NML_lat, lon=NML_lon,  method='nearest'))\n",
    "    NML_OLD_mod= (NML_DS_NEW.sel(lat=NML_lat, lon=NML_lon,  method='nearest'))\n",
    "    print (NML_NEW_mod)\n",
    "    # Select all unique latitudes and longitudes from the dataset.\n",
    "       \n",
    "    \n",
    "    # Create a dataset for the total TGM at each site for the reference and new models.    \n",
    "   # for i in range (len(NorthMidLat)): \n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "        # Calculate the mean and stanadard deviation for each month.        \n",
    "    NML_graph = All_NML_ds.groupby('Month').mean()\n",
    "    NML_graph_SD = All_NML_ds.groupby('Month').std()\n",
    "    # Calculate the mean and standard deviations for the reference and new models.\n",
    "    NML_meanmod_OLD=NML_OLD_mod.mean('concat_dims')\n",
    "    print (NML_meanmod_OLD)\n",
    "    NML_stdevmod_OLD= NML_OLD_mod.std('concat_dims')\n",
    "    \n",
    "    NML_meanmod_NEW= NML_NEW_mod.mean('concat_dims')\n",
    "    NML_stdevmod_NEW= NML_NEW_mod.std('concat_dims')\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    \n",
    "                                            # NORTHERN MID LATITUDES #     \n",
    "    # Convert the time data from a float to a string, specifying months for graph labels\n",
    "    NML_ds.index=pd.to_datetime(NML_ds.Month, format='%m')\n",
    "    # Add a subplot. \n",
    "    ax= plt.subplot(223)\n",
    "    # Plot the observations and their error.\n",
    "    plt.errorbar(NML_ds.Month, NML_graph.Concentration,yerr=NML_graph_SD.Concentration, color='k')\n",
    "    # Plot the reference and new models on the same graph with their errors.\n",
    "   # ax.errorbar(NML_ds.Month,NML_meanmod_OLD ,yerr=NML_stdevmod_OLD, color='Blue')\n",
    "   # ax.errorbar(NML_ds.Month, NML_meanmod_NEW,yerr=NML_stdevmod_NEW, color='Red')\n",
    "    # Label the x and y axis.     \n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('TGM (ng/m3)')\n",
    "    # Add a legend\n",
    "    plt.legend([ 'Observations','Reference Model','New Model' ])\n",
    "    # Add a title.\n",
    "    plt.title(\"Northern Mid Latitudes\")\n",
    "    # Set ticks to every month \n",
    "    ax.set_xticks(NML_ds.Month)\n",
    "    # Set tick labels to month names\n",
    "    ax.set_xticklabels(NML_ds.index.strftime('%b'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hgobs = pd.read_csv('data/TGMSiteMonthly.csv',  skiprows=[0], na_values=(-9999))\n",
    "Hgobs.columns=['SiteID', 'Lat', 'Lon','Month', 'Year', 'Concentration', 'Standard deviation']\n",
    "NorthMidLat= np.array(['MHD', 'UDH', 'KEJ',  'HTW', 'PNY', 'ATN', 'YKV', 'GRB', 'EGB']) \n",
    "newarray=np.zeros(len(NorthMidLat))\n",
    "for i in range (len(NorthMidLat)):\n",
    "    North=Hgobs[Hgobs.SiteID==NorthMidLat[i]].reset_index()\n",
    "    newarray=(North.SiteID.unique())\n",
    "    #print (newarray[0])\n",
    "\n",
    "     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
